{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8024105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils_torch import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9872d4c",
   "metadata": {},
   "source": [
    "# Compilers and Interpreters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a3d80b",
   "metadata": {},
   "source": [
    "## Symbolic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5548989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02aa0233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def fancy_func(a, b, c, d):\n",
      "    e = add(a, b)\n",
      "    f = add(c, d)\n",
      "    g = add(e, f)\n",
      "    return g\n",
      "print(fancy_func(1, 2, 3, 4))\n"
     ]
    }
   ],
   "source": [
    "print(evoke_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d5cff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "y = compile(evoke_(), '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24076e6",
   "metadata": {},
   "source": [
    "## Hybridizing the Sequential Class\n",
    "\n",
    " replacing `Sequential` with `HybridSequential`. We begin by defining a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5af1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0961, -0.1060]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Factory for networks\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2))\n",
    "    return net\n",
    "\n",
    "x = torch.randn(size=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af698eb4",
   "metadata": {},
   "source": [
    "By converting the model using torch.jit.script function, we are able to compile and optimize the computation in the MLP. The modelâ€™s computation result remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d341831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0961, -0.1060]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.jit.script(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ec563",
   "metadata": {},
   "source": [
    "## Acceleration by Hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009b0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark:\n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.timer=Timer()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        print(f\"{self.description}:{self.timer.stop():.4f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e359e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without torchscript:0.0653 sec\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "with Benchmark('Without torchscript'):\n",
    "    for i in range(1000):\n",
    "        net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698d4dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With torchscript:0.0516 sec\n"
     ]
    }
   ],
   "source": [
    "net = torch.jit.script(net)\n",
    "with Benchmark('With torchscript'):\n",
    "    for i in range(1000):\n",
    "        net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e597e8",
   "metadata": {},
   "source": [
    "As is observed in the above results, after an nn.Sequential instance is scripted using the `torch.jit.script` function, computing performance is improved through the use of symbolic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bd801",
   "metadata": {},
   "source": [
    "### Serialization\n",
    "\n",
    " serialize (save) the model and its parameters to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a6b6064",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('my_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ca3ca",
   "metadata": {},
   "source": [
    "# Asynchronous Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9645c91",
   "metadata": {},
   "source": [
    "##  Asynchrony via Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b32bb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy:0.3322 sec\n",
      "torch:0.0003 sec\n"
     ]
    }
   ],
   "source": [
    "device = try_gpu()\n",
    "a = torch.randn(size=(1000, 1000), device=device)\n",
    "b = torch.mm(a, a)\n",
    "\n",
    "with Benchmark('numpy'):\n",
    "    for _ in range(10):\n",
    "        a = np.random.normal(size=(1000, 1000))\n",
    "        b = np.dot(a, a)\n",
    "\n",
    "with Benchmark('torch'):\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e19d0e",
   "metadata": {},
   "source": [
    "The benchmark output via PyTorch is orders of magnitude faster. Reasons:\n",
    "1. NumPy dot product is executed on the CPU processor while PyTorch matrix multiplication is executed on GPU and hence the latter is expected to be much faster. \n",
    "2. By default, GPU operations are asynchronous in PyTorch. Forcing PyTorch to finish all computation prior to returning shows what happened previously: computation is being executed by the backend while the frontend returns control to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743adaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
